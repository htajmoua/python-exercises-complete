# Instructions - Packaging et D√©ploiement

Ce module couvre la cr√©ation, le packaging et la distribution de biblioth√®ques Python, ainsi que la gestion moderne des environnements avec **uv**.

## Partie 1 - Structure d'un package Python

### Exercice 1 - Cr√©er la structure de base

**Cr√©ez** la structure suivante :

```
textanalyzer/
‚îú‚îÄ‚îÄ textanalyzer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py      # Module d'analyse de texte
‚îÇ   ‚îî‚îÄ‚îÄ entities.py      # Module d'extraction d'entit√©s
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_analyzer.py
‚îÇ   ‚îî‚îÄ‚îÄ test_entities.py
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ pyproject.toml
```

**Cr√©ez** `textanalyzer/__init__.py` :

```python
"""
TextAnalyzer - Biblioth√®que d'analyse de texte avec spaCy
"""

__version__ = '0.1.0'
__author__ = 'Votre Nom'

from .analyzer import analyze_text, get_tokens, get_pos_tags
from .entities import extract_entities, extract_persons, extract_locations

__all__ = [
    'analyze_text',
    'get_tokens', 
    'get_pos_tags',
    'extract_entities',
    'extract_persons',
    'extract_locations',
]
```

### Exercice 2 - Cr√©er des modules

**Cr√©ez** `textanalyzer/analyzer.py` :

```python
"""Module d'analyse de texte avec spaCy"""

import spacy
from typing import List, Dict, Any

# Charger le mod√®le spaCy (fran√ßais)
try:
    nlp = spacy.load("fr_core_news_sm")
except OSError:
    print("Mod√®le spaCy non trouv√©. Installez-le avec: python -m spacy download fr_core_news_sm")
    nlp = None


def analyze_text(text: str) -> Dict[str, Any]:
    """
    Analyse compl√®te d'un texte.
    
    Args:
        text: Le texte √† analyser
        
    Returns:
        Dictionnaire avec tokens, lemmes, POS tags et d√©pendances
    """
    if nlp is None:
        raise RuntimeError("Mod√®le spaCy non charg√©")
    
    doc = nlp(text)
    
    return {
        "tokens": [token.text for token in doc],
        "lemmas": [token.lemma_ for token in doc],
        "pos_tags": [token.pos_ for token in doc],
        "n_tokens": len(doc),
        "n_sentences": len(list(doc.sents)),
    }


def get_tokens(text: str) -> List[str]:
    """
    Extrait les tokens d'un texte.
    
    Args:
        text: Le texte √† tokeniser
        
    Returns:
        Liste des tokens
    """
    if nlp is None:
        raise RuntimeError("Mod√®le spaCy non charg√©")
    
    doc = nlp(text)
    return [token.text for token in doc]


def get_pos_tags(text: str) -> List[tuple[str, str]]:
    """
    Extrait les tokens avec leurs POS tags.
    
    Args:
        text: Le texte √† analyser
        
    Returns:
        Liste de tuples (token, pos_tag)
    """
    if nlp is None:
        raise RuntimeError("Mod√®le spaCy non charg√©")
    
    doc = nlp(text)
    return [(token.text, token.pos_) for token in doc]
```

**Cr√©ez** `textanalyzer/entities.py` :

```python
"""Module d'extraction d'entit√©s nomm√©es"""

import spacy
from typing import List, Dict

# Charger le mod√®le spaCy (fran√ßais)
try:
    nlp = spacy.load("fr_core_news_sm")
except OSError:
    print("Mod√®le spaCy non trouv√©. Installez-le avec: python -m spacy download fr_core_news_sm")
    nlp = None


def extract_entities(text: str) -> List[Dict[str, str]]:
    """
    Extrait toutes les entit√©s nomm√©es d'un texte.
    
    Args:
        text: Le texte √† analyser
        
    Returns:
        Liste de dictionnaires avec les entit√©s et leurs types
    """
    if nlp is None:
        raise RuntimeError("Mod√®le spaCy non charg√©")
    
    doc = nlp(text)
    
    return [
        {
            "text": ent.text,
            "label": ent.label_,
            "start": ent.start_char,
            "end": ent.end_char,
        }
        for ent in doc.ents
    ]


def extract_persons(text: str) -> List[str]:
    """
    Extrait les noms de personnes d'un texte.
    
    Args:
        text: Le texte √† analyser
        
    Returns:
        Liste des noms de personnes trouv√©s
    """
    if nlp is None:
        raise RuntimeError("Mod√®le spaCy non charg√©")
    
    doc = nlp(text)
    return [ent.text for ent in doc.ents if ent.label_ == "PER"]


def extract_locations(text: str) -> List[str]:
    """
    Extrait les noms de lieux d'un texte.
    
    Args:
        text: Le texte √† analyser
        
    Returns:
        Liste des lieux trouv√©s
    """
    if nlp is None:
        raise RuntimeError("Mod√®le spaCy non charg√©")
    
    doc = nlp(text)
    return [ent.text for ent in doc.ents if ent.label_ == "LOC"]
```

## Partie 2 - Configuration avec pyproject.toml

### Exercice 3 - Cr√©er pyproject.toml

**Cr√©ez** `pyproject.toml` :

```toml
[build-system]
requires = ["setuptools>=68.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "textanalyzer"
version = "0.1.0"
description = "Biblioth√®que d'analyse de texte avec spaCy"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "Votre Nom", email = "votre.email@example.com"}
]
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Topic :: Text Processing :: Linguistic",
]
dependencies = [
    "spacy>=3.7.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.5.0",
]

[project.urls]
Homepage = "https://github.com/votrenom/textanalyzer"
Documentation = "https://textanalyzer.readthedocs.io"
Repository = "https://github.com/votrenom/textanalyzer"
Issues = "https://github.com/votrenom/textanalyzer/issues"

[tool.black]
line-length = 88
target-version = ['py310', 'py311', 'py312']

[tool.ruff]
line-length = 88
target-version = "py310"
select = ["E", "F", "I", "N", "W", "UP"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --cov=textanalyzer --cov-report=term-missing"
```

### Exercice 4 - Installer en mode d√©veloppement

**Cr√©ez** un environnement virtuel et installez votre package :

```bash
# Cr√©er un environnement virtuel
uv venv

# Activer l'environnement
source .venv/bin/activate  # Mac/Linux
# .venv\Scripts\activate  # Windows

# En mode d√©veloppement (editable)
uv pip install -e .
```

**Testez** l'import :

```python
import textanalyzer
print(textanalyzer.__version__)
from textanalyzer import analyze_text, extract_entities
```

**Installez** le mod√®le spaCy fran√ßais :

```bash
# T√©l√©charger le mod√®le fran√ßais de spaCy
python -m spacy download fr_core_news_sm
```

**Testez** le package :

```python
from textanalyzer import analyze_text, extract_entities

# Analyser un texte
text = "Emmanuel Macron est √† Paris pour rencontrer des dirigeants."
result = analyze_text(text)
print(result)

# Extraire les entit√©s
entities = extract_entities(text)
print(entities)
```

**Installez** les d√©pendances de d√©veloppement :

```bash
# Installer toutes les d√©pendances incluant dev
uv pip install -e ".[dev]"

# Ou ajouter des d√©pendances avec uv
uv add spacy
uv add --dev pytest pytest-cov black ruff mypy
```

**Utilisez** les outils de d√©veloppement :

```bash
# Formater le code avec black
uv run black textanalyzer/

# Linter avec ruff
uv run ruff check textanalyzer/

# Type checking avec mypy
uv run mypy textanalyzer/

# Lancer les tests
uv run pytest

# Avec coverage
uv run pytest --cov=textanalyzer --cov-report=html
```

## Partie 3 - Outils de qualit√© et tests

### Les outils de qualit√© modernes

Le `pyproject.toml` inclut 4 outils essentiels :

1. **pytest** + **pytest-cov** : Framework de tests et mesure de couverture
2. **black** : Formatage automatique du code (opinionated)
3. **ruff** : Linter ultra-rapide (remplace flake8, isort, etc.)
4. **mypy** : V√©rification de types statiques

Ces outils sont configur√©s dans les sections `[tool.*]` du `pyproject.toml`.

### Exercice 5 - Cr√©er des tests

**Cr√©ez** `tests/test_analyzer.py` :

```python
import pytest
from textanalyzer.analyzer import get_tokens, analyze_text


def test_get_tokens():
    """Test de la tokenization"""
    text = "Bonjour le monde"
    tokens = get_tokens(text)
    assert len(tokens) == 3
    assert tokens[0] == "Bonjour"


def test_analyze_text():
    """Test de l'analyse compl√®te"""
    text = "Python est g√©nial."
    result = analyze_text(text)
    
    assert "tokens" in result
    assert "lemmas" in result
    assert "pos_tags" in result
    assert result["n_tokens"] > 0
    assert result["n_sentences"] == 1


def test_analyze_empty_text():
    """Test avec texte vide"""
    result = analyze_text("")
    assert result["n_tokens"] == 0
```

**Cr√©ez** `tests/test_entities.py` :

```python
import pytest
from textanalyzer.entities import extract_entities, extract_persons, extract_locations


def test_extract_entities():
    """Test de l'extraction d'entit√©s"""
    text = "Emmanuel Macron habite √† Paris."
    entities = extract_entities(text)
    
    assert len(entities) > 0
    assert all("text" in ent for ent in entities)
    assert all("label" in ent for ent in entities)


def test_extract_persons():
    """Test de l'extraction de personnes"""
    text = "Marie et Pierre sont √† Lyon."
    persons = extract_persons(text)
    
    # Le r√©sultat peut varier selon le mod√®le
    assert isinstance(persons, list)


def test_extract_locations():
    """Test de l'extraction de lieux"""
    text = "Paris et Lyon sont en France."
    locations = extract_locations(text)
    
    assert isinstance(locations, list)


def test_no_entities():
    """Test sans entit√©s"""
    text = "Le chat mange."
    entities = extract_entities(text)
    assert isinstance(entities, list)
```

**Ex√©cutez** les tests et outils de qualit√© :

```bash
# Ex√©cuter les tests
uv run pytest

# Avec coverage
uv run pytest --cov=textanalyzer --cov-report=html

# Formater le code
uv run black textanalyzer/ tests/

# Linter avec ruff (plus rapide que flake8)
uv run ruff check textanalyzer/ tests/

# R√©parer automatiquement les erreurs ruff
uv run ruff check --fix textanalyzer/

# Type checking avec mypy
uv run mypy textanalyzer/

# Pipeline complet de qualit√©
uv run black . && uv run ruff check . && uv run mypy textanalyzer/ && uv run pytest --cov
```

## Partie 4 - Gestion d'environnements avec uv

### Exercice 6 - Installer uv

**Installez** uv (gestionnaire de packages ultra-rapide) :

```bash
# Mac/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Ou avec pip
pip install uv
```

**V√©rifiez** l'installation :

```bash
uv --version
```

### Exercice 7 - Cr√©er et g√©rer un environnement virtuel

**Cr√©ez** un environnement virtuel :

```bash
# Cr√©er un venv avec uv
uv venv

# Avec une version Python sp√©cifique
uv venv --python 3.11

# Avec un nom personnalis√©
uv venv mon-env
```

**Activez** l'environnement :

```bash
# Mac/Linux
source .venv/bin/activate

# Windows
.venv\Scripts\activate
```

**Installez** des packages :

```bash
# Installer un package (ultra-rapide !)
uv pip install requests

# Installer plusieurs packages
uv pip install requests numpy pandas

# Installer en mode √©ditable (d√©veloppement)
uv pip install -e .

# Avec extras
uv pip install -e ".[dev]"
```

### Exercice 8 - G√©rer les d√©pendances avec pyproject.toml et uv

**Toutes les d√©pendances sont dans pyproject.toml** :

```bash
# Ajouter une d√©pendance (modifie automatiquement pyproject.toml)
uv add requests

# Ajouter avec contrainte de version
uv add "numpy>=1.20,<2.0"

# Ajouter une d√©pendance de d√©veloppement
uv add --dev pytest black

# Retirer une d√©pendance
uv remove requests
```

**Synchroniser l'environnement** :

```bash
# Installer toutes les d√©pendances depuis pyproject.toml
uv sync

# Installer uniquement les d√©pendances de production
uv sync --no-dev
```

**Commandes utiles** :

```bash
# Lister les packages install√©s
uv pip list

# Afficher les infos d'un package
uv pip show requests

# Mettre √† jour toutes les d√©pendances
uv lock --upgrade

# Mettre √† jour une d√©pendance sp√©cifique
uv add --upgrade requests
```

### Exercice 9 - Cr√©er un projet avec uv init

**Initialisez** un nouveau projet :

```bash
# Cr√©er un nouveau projet
uv init mon-projet
cd mon-projet
```

Structure cr√©√©e :

```
mon-projet/
‚îú‚îÄ‚îÄ .venv/           # Environnement virtuel (cr√©√© automatiquement)
‚îú‚îÄ‚îÄ .python-version  # Version Python
‚îú‚îÄ‚îÄ pyproject.toml   # Configuration du projet
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ hello.py         # Script exemple
```

**Le pyproject.toml g√©n√©r√©** :

```toml
[project]
name = "mon-projet"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10"
dependencies = []

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

**Ajoutez** des d√©pendances :

```bash
# Ajouter des d√©pendances de production
uv add requests
uv add "numpy>=1.20,<2.0"

# Ajouter toutes les d√©pendances de d√©veloppement
uv add --dev pytest pytest-cov black ruff mypy

# Le pyproject.toml sera automatiquement mis √† jour
```

**Apr√®s ajout des d√©pendances dev, votre pyproject.toml contiendra** :

```toml
[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.5.0",
]
```

**Ex√©cutez** votre code et les outils de qualit√© :

```bash
# Ex√©cuter un script avec l'environnement activ√©
uv run python hello.py

# Formater le code
uv run black .

# V√©rifier le code avec ruff
uv run ruff check .

# Type checking
uv run mypy .

# Ex√©cuter les tests
uv run pytest

# Tests avec coverage
uv run pytest --cov

# Cha√Æner les commandes (format + lint + test)
uv run black . && uv run ruff check . && uv run pytest
```

## Partie 5 - Distribution sur PyPI

### Exercice 10 - Pr√©parer la distribution

**Cr√©ez** les fichiers n√©cessaires :

**README.md** :

```markdown
# TextAnalyzer üìù

Biblioth√®que Python d'analyse de texte utilisant spaCy pour le traitement du langage naturel.

## Fonctionnalit√©s

- üî§ Tokenization et lemmatisation
- üè∑Ô∏è POS tagging (Part-of-Speech)
- üë§ Extraction d'entit√©s nomm√©es (personnes, lieux, organisations)
- üìä Analyse statistique de texte

## Installation

```bash
pip install textanalyzer
```

Installez le mod√®le spaCy fran√ßais :

```bash
python -m spacy download fr_core_news_sm
```

## Utilisation

### Analyse de texte

```python
from textanalyzer import analyze_text

text = "Emmanuel Macron est le pr√©sident de la France."
result = analyze_text(text)

print(result)
# {
#     'tokens': ['Emmanuel', 'Macron', 'est', 'le', 'pr√©sident', ...],
#     'lemmas': ['Emmanuel', 'Macron', '√™tre', 'le', 'pr√©sident', ...],
#     'pos_tags': ['PROPN', 'PROPN', 'AUX', 'DET', 'NOUN', ...],
#     'n_tokens': 9,
#     'n_sentences': 1
# }
```

### Extraction d'entit√©s

```python
from textanalyzer import extract_entities, extract_persons, extract_locations

text = "Marie habite √† Paris et travaille √† Lyon."

# Toutes les entit√©s
entities = extract_entities(text)

# Seulement les personnes
persons = extract_persons(text)  # ['Marie']

# Seulement les lieux
locations = extract_locations(text)  # ['Paris', 'Lyon']
```

### Tokenization simple

```python
from textanalyzer import get_tokens, get_pos_tags

text = "Python est un langage de programmation."

tokens = get_tokens(text)
# ['Python', 'est', 'un', 'langage', 'de', 'programmation', '.']

pos_tags = get_pos_tags(text)
# [('Python', 'PROPN'), ('est', 'AUX'), ...]
```

## D√©veloppement

```bash
# Cloner le repo
git clone https://github.com/votrenom/textanalyzer
cd textanalyzer

# Cr√©er l'environnement
uv venv
source .venv/bin/activate

# Installer en mode dev
uv pip install -e ".[dev]"

# Lancer les tests
uv run pytest

# Formater le code
uv run black .

# Linter
uv run ruff check .
```

## Licence

MIT
```

**LICENSE** (MIT) :

```
MIT License

Copyright (c) 2024 Votre Nom

Permission is hereby granted, free of charge, to any person obtaining a copy...
```

### Exercice 11 - Construire les distributions

**Installez** les outils avec uv :

```bash
uv pip install build twine
```

**Construisez** :

```bash
# Nettoyer les anciens builds
rm -rf dist/ build/ *.egg-info

# Construire avec uv
uv run python -m build

# V√©rifier les distributions
uv run twine check dist/*
```

### Exercice 12 - Test sur TestPyPI

**1. Cr√©ez** un compte sur https://test.pypi.org/

**2. G√©n√©rez** un token API sur https://test.pypi.org/manage/account/token/

**3. Ajoutez** le token TestPyPI dans `~/.pypirc` (section `[testpypi]`)

**4. Uploadez** :

```bash
# Uploader sur TestPyPI
uv run twine upload --repository testpypi dist/*

# Tester l'installation dans un nouvel environnement
uv pip install --index-url https://test.pypi.org/simple/ textanalyzer

# T√©l√©charger le mod√®le spaCy
python -m spacy download fr_core_news_sm
```

## TP Final - Publier sur PyPI

### Pr√©requis

**1. Cr√©ez un compte sur https://pypi.org/**

**2. G√©n√©rez un token API :**

- Allez sur https://pypi.org/manage/account/token/
- Cliquez sur "Add API token"
- Nom du token : "textanalyzer-upload" (ou autre)
- Scope : "Entire account" (pour le premier upload) ou "Project: textanalyzer" (si le projet existe d√©j√†)
- Copiez le token (il commence par `pypi-AgE...`)
- ‚ö†Ô∏è **Important** : Sauvegardez-le imm√©diatement, il ne sera plus visible !

**3. Configurez l'authentification**

**Cr√©ez** `~/.pypirc` :

```ini
[pypi]
username = __token__
password = pypi-AgExxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

[testpypi]
username = __token__
password = pypi-AgExxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

> **Note** : Remplacez les `xxx` par votre vrai token PyPI

### Checklist avant publication

Avant de publier, v√©rifiez :

- [ ] Tous les tests passent (`uv run pytest`)
- [ ] Code format√© (`uv run black .`)
- [ ] Pas d'erreurs de linting (`uv run ruff check .`)
- [ ] Version correcte dans `pyproject.toml`
- [ ] README.md complet et √† jour
- [ ] LICENSE pr√©sent
- [ ] Build v√©rifi√© (`uv run twine check dist/*`)
- [ ] Test√© sur TestPyPI (optionnel mais recommand√©)

### Publier

```bash
# Uploader sur PyPI r√©el
uv run twine upload dist/*
```

**R√©pondez aux questions** :
- Enter your username: (appuyez sur Entr√©e, le username est d√©j√† dans .pypirc)
- Enter your password: (appuyez sur Entr√©e, le token est d√©j√† dans .pypirc)

‚úÖ **Votre package est maintenant public sur PyPI !**

### V√©rifier

```bash
# Cr√©er un nouvel environnement pour tester
uv venv test-env
source test-env/bin/activate

# Installer depuis PyPI
uv pip install textanalyzer

# Tester
python -c "import textanalyzer; print(textanalyzer.__version__)"
```

### Erreurs courantes et solutions

**Erreur : "The user 'xxx' isn't allowed to upload to project 'textanalyzer'"**
- Le nom est d√©j√† pris sur PyPI
- Solution : Changez le nom dans `pyproject.toml` (ex: `textanalyzer-votreprenom`)

**Erreur : "File already exists"**
- Vous avez d√©j√† upload√© cette version
- Solution : Incr√©mentez la version dans `pyproject.toml` (ex: 0.1.0 ‚Üí 0.1.1)

**Erreur : "Invalid or non-existent authentication"**
- Token incorrect ou expir√©
- Solution : V√©rifiez le token dans `~/.pypirc` ou g√©n√©rez-en un nouveau

**Erreur : "HTTPError: 403 Forbidden"**
- Pas les permissions pour ce projet
- Solution : Utilisez un token avec le bon scope (Entire account pour le premier upload)

## Partie 6 - Configuration avanc√©e

### Exercice 13 - Entry points

**Ajoutez** des scripts CLI dans `pyproject.toml` :

```toml
[project.scripts]
textanalyzer = "textanalyzer.cli:main"
analyze = "textanalyzer.cli:analyze_cli"
```

**Cr√©ez** `textanalyzer/cli.py` :

```python
"""Interface en ligne de commande pour TextAnalyzer"""
import sys
from .analyzer import analyze_text
from .entities import extract_entities


def main():
    """Point d'entr√©e principal"""
    print("TextAnalyzer - Analyse de texte avec spaCy")
    print("Usage: textanalyzer")
    print("       analyze 'votre texte'")


def analyze_cli():
    """Analyser un texte depuis la ligne de commande"""
    if len(sys.argv) < 2:
        print("Usage: analyze 'votre texte'")
        sys.exit(1)
    
    text = " ".join(sys.argv[1:])
    
    # Analyse
    result = analyze_text(text)
    print(f"\nüìä Analyse de texte:")
    print(f"  Tokens: {result['n_tokens']}")
    print(f"  Phrases: {result['n_sentences']}")
    
    # Entit√©s
    entities = extract_entities(text)
    if entities:
        print(f"\nüë§ Entit√©s trouv√©es:")
        for ent in entities:
            print(f"  - {ent['text']} ({ent['label']})")


if __name__ == '__main__':
    analyze_cli()
```

**Installez** et testez :

```bash
# Installer en mode √©ditable
uv pip install -e .

# Ex√©cuter les commandes
textanalyzer

analyze "Emmanuel Macron est √† Paris."
```

### Exercice 14 - Donn√©es de package

**Incluez** des fichiers de donn√©es dans `pyproject.toml` :

```toml
[tool.setuptools.package-data]
textanalyzer = ["data/*.json", "templates/*.html"]
```

**Ou utilisez MANIFEST.in** pour plus de contr√¥le :

```
include README.md
include LICENSE
recursive-include textanalyzer/data *
recursive-include textanalyzer/templates *
```

**Structure recommand√©e** :

```
textanalyzer/
‚îú‚îÄ‚îÄ textanalyzer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ template.html
‚îî‚îÄ‚îÄ pyproject.toml
```

### Exercice 15 - Versioning automatique

**Avec setuptools_scm dans pyproject.toml** :

```bash
uv add --dev setuptools-scm
```

**Modifiez** `pyproject.toml` :

```toml
[build-system]
requires = ["setuptools>=64", "setuptools_scm>=8"]
build-backend = "setuptools.build_meta"

[tool.setuptools_scm]
# Version automatiquement d√©termin√©e depuis Git tags
```

**Cr√©ez** un tag Git :

```bash
git tag v0.1.0
git push --tags
```

La version sera automatiquement d√©termin√©e depuis les tags Git.

## Partie 7 - Buildout (optionnel)

### Exercice 16 - Configurer Buildout

**Cr√©ez** `buildout.cfg` :

```ini
[buildout]
parts = python

[python]
recipe = zc.recipe.egg
eggs = 
    requests
    numpy
interpreter = python
```

**Utilisez** :

```bash
uv pip install zc.buildout
buildout
./bin/python  # Python avec packages install√©s
```

## Checklist de validation

-  Structure de package cr√©√©e
-  ‚úÖ **pyproject.toml** comme unique source de configuration
-  ‚ùå Pas de setup.py (approche moderne)
-  ‚ùå Pas de requirements.txt (tout dans pyproject.toml)
-  **Outils de qualit√© configur√©s** :
   -  ‚úÖ pytest + pytest-cov pour les tests
   -  ‚úÖ black pour le formatage
   -  ‚úÖ ruff pour le linting (remplace flake8)
   -  ‚úÖ mypy pour le type checking
-  Tests √©crits et ex√©cut√©s avec `uv run pytest`
-  Code format√© avec `uv run black`
-  Code lint√© avec `uv run ruff check`
-  Types v√©rifi√©s avec `uv run mypy`
-  uv install√© et environnement virtuel cr√©√©
-  Gestion des d√©pendances avec `uv add` et `uv sync`
-  `uv init` et `uv add --dev` ma√Ætris√©s
-  Pipeline de qualit√© complet (black + ruff + mypy + pytest)
-  Package construit avec `uv run python -m build`
-  Upload√© sur TestPyPI
-  **TP : Publi√© sur PyPI r√©el**
-  Entry points configur√©s dans pyproject.toml
-  Documentation README compl√®te
-  Versioning automatique avec setuptools_scm
